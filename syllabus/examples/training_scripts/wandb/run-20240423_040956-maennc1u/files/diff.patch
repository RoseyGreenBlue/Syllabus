diff --git a/setup.py b/setup.py
index 31e09f2..22a94e8 100644
--- a/setup.py
+++ b/setup.py
@@ -2,7 +2,7 @@ from setuptools import find_packages, setup
 
 
 extras = dict()
-extras['test'] = ['cmake', 'ninja', 'nle>=0.9.0', 'matplotlib>=3.7.1', 'scipy==1.10.0', 'tensorboard>=2.13.0', 'shimmy']
+extras['test'] = ['cmake', 'matplotlib>=3.7.1', 'scipy==1.10.0', 'tensorboard>=2.13.0', 'shimmy']
 extras['docs'] = ['sphinx-tabs', 'sphinxcontrib-spelling', 'furo']
 extras['all'] = extras['test'] + extras['docs']
 
diff --git a/syllabus/examples/training_scripts/cleanrl_procgen_plr.py b/syllabus/examples/training_scripts/cleanrl_procgen_plr.py
index dabcd50..b807304 100644
--- a/syllabus/examples/training_scripts/cleanrl_procgen_plr.py
+++ b/syllabus/examples/training_scripts/cleanrl_procgen_plr.py
@@ -136,7 +136,7 @@ def make_env(env_id, seed, curriculum=None, start_level=0, num_levels=1):
             env = MultiProcessingSyncWrapper(
                 env,
                 curriculum.get_components(),
-                update_on_step=curriculum.requires_step_updates,
+                update_on_step=False,
                 task_space=env.task_space,
             )
         return env
@@ -150,37 +150,31 @@ def wrap_vecenv(vecenv):
     return vecenv
 
 
-def full_level_replay_evaluate(
+def slow_level_replay_evaluate(
     env_name,
     policy,
     num_episodes,
     device,
-    num_levels=1    # Not used
+    num_levels=0
 ):
     policy.eval()
 
     eval_envs = ProcgenEnv(
-        num_envs=args.num_eval_episodes, env_name=env_name, num_levels=1, start_level=0, distribution_mode="easy", paint_vel_info=False
+        num_envs=1, env_name=env_name, num_levels=num_levels, start_level=0, distribution_mode="easy", paint_vel_info=False
     )
     eval_envs = VecExtractDictObs(eval_envs, "rgb")
     eval_envs = wrap_vecenv(eval_envs)
-
-    # Seed environments
-    seeds = [int.from_bytes(os.urandom(3), byteorder="little") for _ in range(num_episodes)]
-    for i, seed in enumerate(seeds):
-        eval_envs.seed(seed, i)
-
     eval_obs, _ = eval_envs.reset()
-    eval_episode_rewards = [-1] * num_episodes
+    eval_episode_rewards = []
 
-    while -1 in eval_episode_rewards:
+    while len(eval_episode_rewards) < num_episodes:
         with torch.no_grad():
             eval_action, _, _, _ = policy.get_action_and_value(torch.Tensor(eval_obs).to(device), deterministic=False)
 
         eval_obs, _, truncs, terms, infos = eval_envs.step(eval_action.cpu().numpy())
         for i, info in enumerate(infos):
-            if 'episode' in info.keys() and eval_episode_rewards[i] == -1:
-                eval_episode_rewards[i] = info['episode']['r']
+            if 'episode' in info.keys():
+                eval_episode_rewards.append(info['episode']['r'])
 
     mean_returns = np.mean(eval_episode_rewards)
     stddev_returns = np.std(eval_episode_rewards)
@@ -251,7 +245,7 @@ if __name__ == "__main__":
         )
         # wandb.run.log_code("./syllabus/examples")
 
-    writer = SummaryWriter(os.path.join(args.logging_dir, "./runs/{run_name}"))
+    writer = SummaryWriter(os.path.join(args.logging_dir, f"./runs/{run_name}"))
     writer.add_text(
         "hyperparameters",
         "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
@@ -271,7 +265,9 @@ if __name__ == "__main__":
     if args.curriculum:
         sample_env = openai_gym.make(f"procgen-{args.env_id}-v0")
         sample_env = GymV21CompatibilityV0(env=sample_env)
-        sample_env = ProcgenTaskWrapper(sample_env, args.env_id, seed=args.seed)
+				# code to edit
+        # sample_env = ProcgenTaskWrapper(sample_env, args.env_id, seed=args.seed)
+        sample_env = MinigridTaskWrapper(sample_env, args.env_id, seed=args.seed)
 
         # Intialize Curriculum Method
         if args.curriculum_method == "plr":
@@ -485,13 +481,13 @@ if __name__ == "__main__":
         mean_eval_returns, stddev_eval_returns, normalized_mean_eval_returns = level_replay_evaluate(
             args.env_id, agent, args.num_eval_episodes, device, num_levels=0
         )
-        full_mean_eval_returns, full_stddev_eval_returns, full_normalized_mean_eval_returns = full_level_replay_evaluate(
+        slow_mean_eval_returns, slow_stddev_eval_returns, slow_normalized_mean_eval_returns = slow_level_replay_evaluate(
             args.env_id, agent, args.num_eval_episodes, device, num_levels=0
         )
         mean_train_returns, stddev_train_returns, normalized_mean_train_returns = level_replay_evaluate(
             args.env_id, agent, args.num_eval_episodes, device, num_levels=200
         )
-        full_mean_train_returns, full_stddev_train_returns, full_normalized_mean_train_returns = full_level_replay_evaluate(
+        slow_mean_train_returns, slow_stddev_train_returns, slow_normalized_mean_train_returns = level_replay_evaluate(
             args.env_id, agent, args.num_eval_episodes, device, num_levels=200
         )
 
@@ -510,17 +506,17 @@ if __name__ == "__main__":
 
         writer.add_scalar("test_eval/mean_episode_return", mean_eval_returns, global_step)
         writer.add_scalar("test_eval/normalized_mean_eval_return", normalized_mean_eval_returns, global_step)
-        writer.add_scalar("test_eval/stddev_eval_return", stddev_eval_returns, global_step)
-        writer.add_scalar("test_eval/full_mean_episode_return", full_mean_eval_returns, global_step)
-        writer.add_scalar("test_eval/full_normalized_mean_eval_return", full_normalized_mean_eval_returns, global_step)
-        writer.add_scalar("test_eval/full_stddev_eval_return", full_stddev_eval_returns, global_step)
+        writer.add_scalar("test_eval/stddev_eval_return", mean_eval_returns, global_step)
+        writer.add_scalar("test_eval/slow_mean_episode_return", slow_mean_eval_returns, global_step)
+        writer.add_scalar("test_eval/slow_normalized_mean_eval_return", slow_normalized_mean_eval_returns, global_step)
+        writer.add_scalar("test_eval/slow_stddev_eval_return", slow_mean_eval_returns, global_step)
 
         writer.add_scalar("train_eval/mean_episode_return", mean_train_returns, global_step)
         writer.add_scalar("train_eval/normalized_mean_train_return", normalized_mean_train_returns, global_step)
-        writer.add_scalar("train_eval/stddev_train_return", stddev_train_returns, global_step)
-        writer.add_scalar("train_eval/full_mean_episode_return", full_mean_train_returns, global_step)
-        writer.add_scalar("train_eval/full_normalized_mean_train_return", full_normalized_mean_train_returns, global_step)
-        writer.add_scalar("train_eval/full_stddev_train_return", full_stddev_train_returns, global_step)
+        writer.add_scalar("train_eval/stddev_train_return", mean_train_returns, global_step)
+        writer.add_scalar("train_eval/slow_mean_episode_return", slow_mean_train_returns, global_step)
+        writer.add_scalar("train_eval/slow_normalized_mean_train_return", slow_normalized_mean_train_returns, global_step)
+        writer.add_scalar("train_eval/slow_stddev_train_return", slow_mean_train_returns, global_step)
 
         writer.add_scalar("curriculum/completed_episodes", completed_episodes, step)
 
