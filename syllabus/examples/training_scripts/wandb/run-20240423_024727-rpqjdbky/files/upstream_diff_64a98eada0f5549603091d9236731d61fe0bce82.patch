diff --git a/setup.py b/setup.py
index 31e09f2..22a94e8 100644
--- a/setup.py
+++ b/setup.py
@@ -2,7 +2,7 @@ from setuptools import find_packages, setup
 
 
 extras = dict()
-extras['test'] = ['cmake', 'ninja', 'nle>=0.9.0', 'matplotlib>=3.7.1', 'scipy==1.10.0', 'tensorboard>=2.13.0', 'shimmy']
+extras['test'] = ['cmake', 'matplotlib>=3.7.1', 'scipy==1.10.0', 'tensorboard>=2.13.0', 'shimmy']
 extras['docs'] = ['sphinx-tabs', 'sphinxcontrib-spelling', 'furo']
 extras['all'] = extras['test'] + extras['docs']
 
diff --git a/syllabus/curricula/plr/plr_wrapper.py b/syllabus/curricula/plr/plr_wrapper.py
index 9515df4..f89828b 100644
--- a/syllabus/curricula/plr/plr_wrapper.py
+++ b/syllabus/curricula/plr/plr_wrapper.py
@@ -312,10 +312,10 @@ class PrioritizedLevelReplay(Curriculum):
         """
         Log the task distribution to the provided tensorboard writer.
         """
-        super().log_metrics(writer, step)
+        # super().log_metrics(writer, step)
         metrics = self._task_sampler.metrics()
         writer.add_scalar("curriculum/proportion_seen", metrics["proportion_seen"], step)
         writer.add_scalar("curriculum/score", metrics["score"], step)
-        for task in list(self.task_space.tasks)[:10]:
-            writer.add_scalar(f"curriculum/task_{task - 1}_score", metrics["task_scores"][task - 1], step)
-            writer.add_scalar(f"curriculum/task_{task - 1}_staleness", metrics["task_staleness"][task - 1], step)
+        # for task in list(self.task_space.tasks)[:10]:
+        #     writer.add_scalar(f"curriculum/task_{task - 1}_score", metrics["task_scores"][task - 1], step)
+        #     writer.add_scalar(f"curriculum/task_{task - 1}_staleness", metrics["task_staleness"][task - 1], step)
diff --git a/syllabus/examples/training_scripts/cleanrl_procgen_centralplr.py b/syllabus/examples/training_scripts/cleanrl_procgen_centralplr.py
index a6d469e..8f1cc34 100644
--- a/syllabus/examples/training_scripts/cleanrl_procgen_centralplr.py
+++ b/syllabus/examples/training_scripts/cleanrl_procgen_centralplr.py
@@ -14,6 +14,7 @@ import gym as openai_gym
 import gymnasium as gym
 import numpy as np
 import procgen  # noqa: F401
+from procgen import ProcgenEnv
 import torch
 import torch.nn as nn
 import torch.optim as optim
@@ -21,10 +22,10 @@ from shimmy.openai_gym_compatibility import GymV21CompatibilityV0
 from torch.utils.tensorboard import SummaryWriter
 
 from syllabus.core import MultiProcessingSyncWrapper, make_multiprocessing_curriculum
-from syllabus.curricula import DomainRandomization, LearningProgressCurriculum, CentralizedPrioritizedLevelReplay
+from syllabus.curricula import PrioritizedLevelReplay, DomainRandomization, LearningProgressCurriculum, SequentialCurriculum
 from syllabus.examples.models import ProcgenAgent
 from syllabus.examples.task_wrappers import ProcgenTaskWrapper
-from syllabus.examples.utils.vecenv import VecMonitor, VecNormalize
+from syllabus.examples.utils.vecenv import VecMonitor, VecNormalize, VecExtractDictObs
 
 
 def parse_args():
@@ -46,6 +47,8 @@ def parse_args():
                         help="the entity (team) of wandb's project")
     parser.add_argument("--capture-video", type=lambda x: bool(strtobool(x)), default=False, nargs="?", const=True,
                         help="weather to capture videos of the agent performances (check out `videos` folder)")
+    parser.add_argument("--logging-dir", type=str, default=".",
+                        help="the base directory for logging and wandb storage.")
 
     # Algorithm specific arguments
     parser.add_argument("--env-id", type=str, default="starpilot",
@@ -124,15 +127,15 @@ PROCGEN_RETURN_BOUNDS = {
 }
 
 
-def make_env(env_id, seed, curriculum_components=None, start_level=0, num_levels=1):
+def make_env(env_id, seed, curriculum=None, start_level=0, num_levels=1):
     def thunk():
         env = openai_gym.make(f"procgen-{env_id}-v0", distribution_mode="easy", start_level=start_level, num_levels=num_levels)
         env = GymV21CompatibilityV0(env=env)
-        env = ProcgenTaskWrapper(env, env_id, seed=seed)
-        if curriculum_components is not None:
+        if curriculum is not None:
+            env = ProcgenTaskWrapper(env, env_id, seed=seed)
             env = MultiProcessingSyncWrapper(
                 env,
-                curriculum_components,
+                curriculum.get_components(),
                 update_on_step=False,
                 task_space=env.task_space,
             )
@@ -147,7 +150,7 @@ def wrap_vecenv(vecenv):
     return vecenv
 
 
-def level_replay_evaluate(
+def slow_level_replay_evaluate(
     env_name,
     policy,
     num_episodes,
@@ -155,28 +158,24 @@ def level_replay_evaluate(
     num_levels=0
 ):
     policy.eval()
-    eval_envs = gym.vector.SyncVectorEnv(
-        [
-            make_env(args.env_id, args.seed + i, task_queue, update_queue, num_levels=num_levels)
-            for i in range(1)
-        ]
+
+    eval_envs = ProcgenEnv(
+        num_envs=1, env_name=env_name, num_levels=num_levels, start_level=0, distribution_mode="easy", paint_vel_info=False
     )
+    eval_envs = VecExtractDictObs(eval_envs, "rgb")
     eval_envs = wrap_vecenv(eval_envs)
-
-    eval_episode_rewards = []
     eval_obs, _ = eval_envs.reset()
+    eval_episode_rewards = []
 
     while len(eval_episode_rewards) < num_episodes:
         with torch.no_grad():
             eval_action, _, _, _ = policy.get_action_and_value(torch.Tensor(eval_obs).to(device), deterministic=False)
 
-        eval_obs, _, truncs, terms, infos = eval_envs.step(np.array([eval_action.cpu().numpy()]))
-
-        for info in infos:
+        eval_obs, _, truncs, terms, infos = eval_envs.step(eval_action.cpu().numpy())
+        for i, info in enumerate(infos):
             if 'episode' in info.keys():
                 eval_episode_rewards.append(info['episode']['r'])
 
-    eval_envs.close()
     mean_returns = np.mean(eval_episode_rewards)
     stddev_returns = np.std(eval_episode_rewards)
     env_min, env_max = PROCGEN_RETURN_BOUNDS[args.env_id]
@@ -185,8 +184,7 @@ def level_replay_evaluate(
     return mean_returns, stddev_returns, normalized_mean_returns
 
 
-def fast_level_replay_evaluate(
-    eval_envs,
+def level_replay_evaluate(
     env_name,
     policy,
     num_episodes,
@@ -194,9 +192,13 @@ def fast_level_replay_evaluate(
     num_levels=0
 ):
     policy.eval()
-    possible_seeds = np.arange(0, num_levels + 1)
-    eval_obs, _ = eval_envs.reset(seed=list(np.random.choice(possible_seeds, size=num_episodes)))
 
+    eval_envs = ProcgenEnv(
+        num_envs=args.num_eval_episodes, env_name=env_name, num_levels=num_levels, start_level=0, distribution_mode="easy", paint_vel_info=False
+    )
+    eval_envs = VecExtractDictObs(eval_envs, "rgb")
+    eval_envs = wrap_vecenv(eval_envs)
+    eval_obs, _ = eval_envs.reset()
     eval_episode_rewards = [-1] * num_episodes
 
     while -1 in eval_episode_rewards:
@@ -231,10 +233,11 @@ if __name__ == "__main__":
             name=run_name,
             monitor_gym=True,
             save_code=True,
-            # dir="/fs/nexus-scratch/rsulli/"
+            dir=args.logging_dir
         )
-        wandb.run.log_code("./syllabus/examples")
-    writer = SummaryWriter(f"./runs/{run_name}")
+        # wandb.run.log_code("./syllabus/examples")
+
+    writer = SummaryWriter(os.path.join(args.logging_dir, "./runs/{run_name}"))
     writer.add_text(
         "hyperparameters",
         "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
@@ -250,7 +253,7 @@ if __name__ == "__main__":
     print("Device:", device)
 
     # Curriculum setup
-    task_queue = update_queue = None
+    curriculum = None
     if args.curriculum:
         sample_env = openai_gym.make(f"procgen-{args.env_id}-v0")
         sample_env = GymV21CompatibilityV0(env=sample_env)
@@ -273,6 +276,16 @@ if __name__ == "__main__":
         elif args.curriculum_method == "lp":
             print("Using learning progress.")
             curriculum = LearningProgressCurriculum(sample_env.task_space)
+        elif args.curriculum_method == "sq":
+            print("Using sequential curriculum.")
+            curricula = []
+            stopping = []
+            for i in range(199):
+                curricula.append(i + 1)
+                stopping.append("steps>=50000")
+                curricula.append(list(range(i + 1)))
+                stopping.append("steps>=50000")
+            curriculum = SequentialCurriculum(curricula, stopping[:-1], sample_env.task_space)
         else:
             raise ValueError(f"Unknown curriculum method {args.curriculum_method}")
         curriculum = make_multiprocessing_curriculum(curriculum)
@@ -285,7 +298,7 @@ if __name__ == "__main__":
             make_env(
                 args.env_id,
                 args.seed + i,
-                curriculum_components=curriculum.get_components() if args.curriculum else None,
+                curriculum=curriculum if args.curriculum else None,
                 num_levels=1 if args.curriculum else 0
             )
             for i in range(args.num_envs)
@@ -293,22 +306,6 @@ if __name__ == "__main__":
     )
     envs = wrap_vecenv(envs)
 
-    test_eval_envs = gym.vector.AsyncVectorEnv(
-        [
-            make_env(args.env_id, args.seed + i, num_levels=0)
-            for i in range(args.num_eval_episodes)
-        ]
-    )
-    test_eval_envs = wrap_vecenv(test_eval_envs)
-
-    train_eval_envs = gym.vector.AsyncVectorEnv(
-        [
-            make_env(args.env_id, args.seed + i, num_levels=200)
-            for i in range(args.num_eval_episodes)
-        ]
-    )
-    train_eval_envs = wrap_vecenv(train_eval_envs)
-
     assert isinstance(envs.single_action_space, gym.spaces.Discrete), "only discrete action space is supported"
     print("Creating agent")
     agent = ProcgenAgent(
@@ -369,6 +366,8 @@ if __name__ == "__main__":
                     print(f"global_step={global_step}, episodic_return={item['episode']['r']}")
                     writer.add_scalar("charts/episodic_return", item["episode"]["r"], global_step)
                     writer.add_scalar("charts/episodic_length", item["episode"]["l"], global_step)
+                    if curriculum is not None:
+                        curriculum.log_metrics(writer, global_step)
                     break
 
             # Syllabus curriculum update
@@ -388,8 +387,6 @@ if __name__ == "__main__":
                     },
                 }
                 curriculum.update(update)
-            #if args.curriculum:
-            #    curriculum.log_metrics(writer, global_step)
 
         # bootstrap value if not done
         with torch.no_grad():
@@ -487,8 +484,18 @@ if __name__ == "__main__":
         explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y
 
         # Evaluate agent
-        mean_eval_returns, stddev_eval_returns, normalized_mean_eval_returns = fast_level_replay_evaluate(test_eval_envs, args.env_id, agent, args.num_eval_episodes, device, num_levels=0)
-        mean_train_returns, stddev_train_returns, normalized_mean_train_returns = fast_level_replay_evaluate(train_eval_envs, args.env_id, agent, args.num_eval_episodes, device, num_levels=200)
+        mean_eval_returns, stddev_eval_returns, normalized_mean_eval_returns = level_replay_evaluate(
+            args.env_id, agent, args.num_eval_episodes, device, num_levels=0
+        )
+        slow_mean_eval_returns, slow_stddev_eval_returns, slow_normalized_mean_eval_returns = slow_level_replay_evaluate(
+            args.env_id, agent, args.num_eval_episodes, device, num_levels=0
+        )
+        mean_train_returns, stddev_train_returns, normalized_mean_train_returns = level_replay_evaluate(
+            args.env_id, agent, args.num_eval_episodes, device, num_levels=200
+        )
+        slow_mean_train_returns, slow_stddev_train_returns, slow_normalized_mean_train_returns = level_replay_evaluate(
+            args.env_id, agent, args.num_eval_episodes, device, num_levels=200
+        )
 
         # TRY NOT TO MODIFY: record rewards for plotting purposes
         writer.add_scalar("charts/learning_rate", optimizer.param_groups[0]["lr"], global_step)
@@ -502,12 +509,21 @@ if __name__ == "__main__":
         writer.add_scalar("losses/explained_variance", explained_var, global_step)
         print("SPS:", int(global_step / (time.time() - start_time)))
         writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
+
         writer.add_scalar("test_eval/mean_episode_return", mean_eval_returns, global_step)
         writer.add_scalar("test_eval/normalized_mean_eval_return", normalized_mean_eval_returns, global_step)
         writer.add_scalar("test_eval/stddev_eval_return", mean_eval_returns, global_step)
+        writer.add_scalar("test_eval/slow_mean_episode_return", slow_mean_eval_returns, global_step)
+        writer.add_scalar("test_eval/slow_normalized_mean_eval_return", slow_normalized_mean_eval_returns, global_step)
+        writer.add_scalar("test_eval/slow_stddev_eval_return", slow_mean_eval_returns, global_step)
+
         writer.add_scalar("train_eval/mean_episode_return", mean_train_returns, global_step)
         writer.add_scalar("train_eval/normalized_mean_train_return", normalized_mean_train_returns, global_step)
         writer.add_scalar("train_eval/stddev_train_return", mean_train_returns, global_step)
+        writer.add_scalar("train_eval/slow_mean_episode_return", slow_mean_train_returns, global_step)
+        writer.add_scalar("train_eval/slow_normalized_mean_train_return", slow_normalized_mean_train_returns, global_step)
+        writer.add_scalar("train_eval/slow_stddev_train_return", slow_mean_train_returns, global_step)
+
         writer.add_scalar("curriculum/completed_episodes", completed_episodes, step)
 
     envs.close()
diff --git a/syllabus/examples/training_scripts/cleanrl_procgen_plr.py b/syllabus/examples/training_scripts/cleanrl_procgen_plr.py
index e13c22e..d2d54b5 100644
--- a/syllabus/examples/training_scripts/cleanrl_procgen_plr.py
+++ b/syllabus/examples/training_scripts/cleanrl_procgen_plr.py
@@ -14,6 +14,7 @@ import gym as openai_gym
 import gymnasium as gym
 import numpy as np
 import procgen  # noqa: F401
+from procgen import ProcgenEnv
 import torch
 import torch.nn as nn
 import torch.optim as optim
@@ -24,7 +25,7 @@ from syllabus.core import MultiProcessingSyncWrapper, make_multiprocessing_curri
 from syllabus.curricula import PrioritizedLevelReplay, DomainRandomization, LearningProgressCurriculum, SequentialCurriculum
 from syllabus.examples.models import ProcgenAgent
 from syllabus.examples.task_wrappers import ProcgenTaskWrapper
-from syllabus.examples.utils.vecenv import VecMonitor, VecNormalize
+from syllabus.examples.utils.vecenv import VecMonitor, VecNormalize, VecExtractDictObs
 
 
 def parse_args():
@@ -126,18 +127,17 @@ PROCGEN_RETURN_BOUNDS = {
 }
 
 
-def make_env(env_id, seed, curriculum_components=None, start_level=0, num_levels=1):
+def make_env(env_id, seed, curriculum=None, start_level=0, num_levels=1):
     def thunk():
         env = openai_gym.make(f"procgen-{env_id}-v0", distribution_mode="easy", start_level=start_level, num_levels=num_levels)
         env = GymV21CompatibilityV0(env=env)
-        env = ProcgenTaskWrapper(env, env_id, seed=seed)
-        if curriculum_components is not None:
+        if curriculum is not None:
+            env = ProcgenTaskWrapper(env, env_id, seed=seed)
             env = MultiProcessingSyncWrapper(
                 env,
-                curriculum_components,
+                curriculum.get_components(),
                 update_on_step=False,
                 task_space=env.task_space,
-                buffer_size=4,
             )
         return env
     return thunk
@@ -150,7 +150,7 @@ def wrap_vecenv(vecenv):
     return vecenv
 
 
-def level_replay_evaluate(
+def slow_level_replay_evaluate(
     env_name,
     policy,
     num_episodes,
@@ -158,28 +158,24 @@ def level_replay_evaluate(
     num_levels=0
 ):
     policy.eval()
-    eval_envs = gym.vector.SyncVectorEnv(
-        [
-            make_env(args.env_id, args.seed + i, task_queue, update_queue, num_levels=num_levels)
-            for i in range(1)
-        ]
+
+    eval_envs = ProcgenEnv(
+        num_envs=1, env_name=env_name, num_levels=num_levels, start_level=0, distribution_mode="easy", paint_vel_info=False
     )
+    eval_envs = VecExtractDictObs(eval_envs, "rgb")
     eval_envs = wrap_vecenv(eval_envs)
-
-    eval_episode_rewards = []
     eval_obs, _ = eval_envs.reset()
+    eval_episode_rewards = []
 
     while len(eval_episode_rewards) < num_episodes:
         with torch.no_grad():
             eval_action, _, _, _ = policy.get_action_and_value(torch.Tensor(eval_obs).to(device), deterministic=False)
 
-        eval_obs, _, truncs, terms, infos = eval_envs.step(np.array([eval_action.cpu().numpy()]))
-
-        for info in infos:
+        eval_obs, _, truncs, terms, infos = eval_envs.step(eval_action.cpu().numpy())
+        for i, info in enumerate(infos):
             if 'episode' in info.keys():
                 eval_episode_rewards.append(info['episode']['r'])
 
-    eval_envs.close()
     mean_returns = np.mean(eval_episode_rewards)
     stddev_returns = np.std(eval_episode_rewards)
     env_min, env_max = PROCGEN_RETURN_BOUNDS[args.env_id]
@@ -188,8 +184,7 @@ def level_replay_evaluate(
     return mean_returns, stddev_returns, normalized_mean_returns
 
 
-def fast_level_replay_evaluate(
-    eval_envs,
+def level_replay_evaluate(
     env_name,
     policy,
     num_episodes,
@@ -198,15 +193,12 @@ def fast_level_replay_evaluate(
 ):
     policy.eval()
 
-    # Choose evaluation seeds
-    if num_levels == 0:
-        seeds = np.random.randint(0, 2 ** 16 - 1, size=num_episodes)
-    else:
-        seeds = np.random.choice(np.arange(0, num_levels), size=num_episodes)
-
-    seed_envs = [(int(seed), env) for seed, env in zip(seeds, range(num_episodes))]
-    eval_obs, _ = eval_envs.reset(seed=seed_envs)
-
+    eval_envs = ProcgenEnv(
+        num_envs=args.num_eval_episodes, env_name=env_name, num_levels=num_levels, start_level=0, distribution_mode="easy", paint_vel_info=False
+    )
+    eval_envs = VecExtractDictObs(eval_envs, "rgb")
+    eval_envs = wrap_vecenv(eval_envs)
+    eval_obs, _ = eval_envs.reset()
     eval_episode_rewards = [-1] * num_episodes
 
     while -1 in eval_episode_rewards:
@@ -251,7 +243,7 @@ if __name__ == "__main__":
             save_code=True,
             dir=args.logging_dir
         )
-        wandb.run.log_code(os.path.join(args.logging_dir, "/syllabus/examples"))
+        # wandb.run.log_code("./syllabus/examples")
 
     writer = SummaryWriter(os.path.join(args.logging_dir, "./runs/{run_name}"))
     writer.add_text(
@@ -316,7 +308,7 @@ if __name__ == "__main__":
             make_env(
                 args.env_id,
                 args.seed + i,
-                curriculum_components=curriculum.get_components() if args.curriculum else None,
+                curriculum=curriculum if args.curriculum else None,
                 num_levels=1 if args.curriculum else 0
             )
             for i in range(args.num_envs)
@@ -324,22 +316,6 @@ if __name__ == "__main__":
     )
     envs = wrap_vecenv(envs)
 
-    test_eval_envs = gym.vector.AsyncVectorEnv(
-        [
-            make_env(args.env_id, args.seed + i, num_levels=0)
-            for i in range(args.num_eval_episodes)
-        ]
-    )
-    test_eval_envs = wrap_vecenv(test_eval_envs)
-
-    train_eval_envs = gym.vector.AsyncVectorEnv(
-        [
-            make_env(args.env_id, args.seed + i, num_levels=200)
-            for i in range(args.num_eval_episodes)
-        ]
-    )
-    train_eval_envs = wrap_vecenv(train_eval_envs)
-
     assert isinstance(envs.single_action_space, gym.spaces.Discrete), "only discrete action space is supported"
     print("Creating agent")
     agent = ProcgenAgent(
@@ -500,8 +476,18 @@ if __name__ == "__main__":
         explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y
 
         # Evaluate agent
-        mean_eval_returns, stddev_eval_returns, normalized_mean_eval_returns = fast_level_replay_evaluate(test_eval_envs, args.env_id, agent, args.num_eval_episodes, device, num_levels=0)
-        mean_train_returns, stddev_train_returns, normalized_mean_train_returns = fast_level_replay_evaluate(train_eval_envs, args.env_id, agent, args.num_eval_episodes, device, num_levels=200)
+        mean_eval_returns, stddev_eval_returns, normalized_mean_eval_returns = level_replay_evaluate(
+            args.env_id, agent, args.num_eval_episodes, device, num_levels=0
+        )
+        slow_mean_eval_returns, slow_stddev_eval_returns, slow_normalized_mean_eval_returns = slow_level_replay_evaluate(
+            args.env_id, agent, args.num_eval_episodes, device, num_levels=0
+        )
+        mean_train_returns, stddev_train_returns, normalized_mean_train_returns = level_replay_evaluate(
+            args.env_id, agent, args.num_eval_episodes, device, num_levels=200
+        )
+        slow_mean_train_returns, slow_stddev_train_returns, slow_normalized_mean_train_returns = level_replay_evaluate(
+            args.env_id, agent, args.num_eval_episodes, device, num_levels=200
+        )
 
         # TRY NOT TO MODIFY: record rewards for plotting purposes
         writer.add_scalar("charts/learning_rate", optimizer.param_groups[0]["lr"], global_step)
@@ -515,12 +501,21 @@ if __name__ == "__main__":
         writer.add_scalar("losses/explained_variance", explained_var, global_step)
         print("SPS:", int(global_step / (time.time() - start_time)))
         writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
+
         writer.add_scalar("test_eval/mean_episode_return", mean_eval_returns, global_step)
         writer.add_scalar("test_eval/normalized_mean_eval_return", normalized_mean_eval_returns, global_step)
         writer.add_scalar("test_eval/stddev_eval_return", mean_eval_returns, global_step)
+        writer.add_scalar("test_eval/slow_mean_episode_return", slow_mean_eval_returns, global_step)
+        writer.add_scalar("test_eval/slow_normalized_mean_eval_return", slow_normalized_mean_eval_returns, global_step)
+        writer.add_scalar("test_eval/slow_stddev_eval_return", slow_mean_eval_returns, global_step)
+
         writer.add_scalar("train_eval/mean_episode_return", mean_train_returns, global_step)
         writer.add_scalar("train_eval/normalized_mean_train_return", normalized_mean_train_returns, global_step)
         writer.add_scalar("train_eval/stddev_train_return", mean_train_returns, global_step)
+        writer.add_scalar("train_eval/slow_mean_episode_return", slow_mean_train_returns, global_step)
+        writer.add_scalar("train_eval/slow_normalized_mean_train_return", slow_normalized_mean_train_returns, global_step)
+        writer.add_scalar("train_eval/slow_stddev_train_return", slow_mean_train_returns, global_step)
+
         writer.add_scalar("curriculum/completed_episodes", completed_episodes, step)
 
     envs.close()
diff --git a/syllabus/examples/utils/vecenv.py b/syllabus/examples/utils/vecenv.py
index 6e5a0a9..af3b187 100644
--- a/syllabus/examples/utils/vecenv.py
+++ b/syllabus/examples/utils/vecenv.py
@@ -1,7 +1,6 @@
 import time
 from collections import deque
 
-import gym
 import numpy as np
 
 
@@ -154,12 +153,20 @@ class VecEnvObservationWrapper(VecEnvWrapper):
         pass
 
     def reset(self):
-        obs, infos = self.venv.reset()
+        outputs = self.venv.reset()
+        if len(outputs) == 2:
+            obs, infos = outputs
+        else:
+            obs, infos = outputs, {}
         return self.process(obs), infos
 
     def step_wait(self):
-        print(self.venv)
-        obs, rews, terms, truncs, infos = self.venv.step_wait()
+        env_outputs = self.venv.step_wait()
+        if len(env_outputs) == 4:
+            obs, rews, terms, infos = env_outputs
+            truncs = np.zeros_like(terms)
+        else:
+            obs, rews, terms, truncs, infos = env_outputs
         return self.process(obs), rews, terms, truncs, infos
 
 
@@ -209,7 +216,10 @@ class VecNormalize(VecEnvWrapper):
 
     def reset(self, seed=None):
         self.ret = np.zeros(self.num_envs)
-        obs, infos = self.venv.reset(seed=seed)
+        if seed is not None:
+            obs, infos = self.venv.reset(seed=seed)
+        else:
+            obs, infos = self.venv.reset()
         return self._obfilt(obs), infos
 
 
@@ -228,7 +238,10 @@ class VecMonitor(VecEnvWrapper):
             self.eplen_buf = deque([], maxlen=keep_buf)
 
     def reset(self, seed=None):
-        obs, infos = self.venv.reset(seed=seed)
+        if seed is not None:
+            obs, infos = self.venv.reset(seed=seed)
+        else:
+            obs, infos = self.venv.reset()
         self.eprets = np.zeros(self.num_envs, 'f')
         self.eplens = np.zeros(self.num_envs, 'i')
         return obs, infos
@@ -239,7 +252,8 @@ class VecMonitor(VecEnvWrapper):
         self.eprets += rews
         self.eplens += 1
         # Convert dict of lists to list of dicts
-        infos = [dict(zip(infos, t)) for t in zip(*infos.values())]
+        if isinstance(infos, dict):
+            infos = [dict(zip(infos, t)) for t in zip(*infos.values())]
         newinfos = list(infos[:])
         for i in range(len(dones)):
             if dones[i]:
